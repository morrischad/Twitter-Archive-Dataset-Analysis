{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chad Morris Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**After visually and programmatically browsing and assessing the datasets I gathered the following quality and tidiness related issues were apparent:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Quality issues\n",
    "1. retweeted_status_id, retweeted_status_user_id, and retweeted_statsus_timestamp columns in dogs_df are columns related to duplicated data, not originals. These columns are not needed since I only want original tweets but simply reposts.\n",
    "\n",
    "2. Timestamp columns in dogs_df of type object/string when it should be of type datetime.\n",
    "\n",
    "3. The dog stages in dogs_df are objects/strings when they really should be categories.\n",
    "\n",
    "4. The source column in dog_df contains some of the HTML anchor tag syntax still instead of just the text.\n",
    "\n",
    "5. tweet_id, in the dogs_df and image_pred_df, and img_num columns in the image_pred_df dataframe are of type int64 when they should be objects/strings.\n",
    "\n",
    "6. p1, p2, p3 column values in image_pred_df have inconsistent casing and should have spaces instead of underscores.\n",
    "\n",
    "7. Some of the denominators in the rating_denominator column weren't properly extracted and thus are not set to their proper values.\n",
    "\n",
    "8. Some dogs\n",
    "\n",
    "### Tidiness Issues\n",
    "1. Doggo, floofer, pupper, puppo columns in dogs_df should be one column instead of multiple columns.\n",
    "\n",
    "2. All three datasets need to be joined together to form a complete dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to carry out my analysis of the data related to the WeRateDogs Twitter archive some wrangling was initially needed to gather the relevant datasets, asses those datasets, do any necessary cleaning, and finally engage in analyzing the data.\n",
    "\n",
    "Three datasets in total were used, each of which required different methods for gathering. The original `twitter_archive_enhanced.csv` was provided to me at the start and thus simply had to be downloaded. The second dataset, `image_predicitons.tsv` required a slightly more hands on approach. I programmatically downloaded the dataset by utilizing the requests library in python and extracting the contents from the following link:\n",
    "\n",
    "https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "\n",
    "The third and final dataset required the most nuanced approach. I utilized Twitter's Tweepy API in order to gather two pieces of information: the retweet and favorite counts from the WeRateDogs twitter archive. Using Tweepy, I used the tweet IDs to query the JSON data for every tweet and store the entirety of the JSON data in a text file. Then read each line of the text file into a PANDAS dataframe. \n",
    "\n",
    "After gathering I visually assessed the data to become familiar and build my intuition about the information. After visually assessment I did some standard programmatic assessment and made to see what information stood out, what information seemed erroneous, and what information could be used for further analysis whether independent of or in relation to other variable(s). I also wanted to see what an initial exploratory analysis would reveal to me.\n",
    "\n",
    "Cleaning the data soon came after. Errors with casing, representation, as well as erroneous datatypes were the most common issues that needed to be tackled. Due to the nature of how WeRateDogs does their rating for dogs, the usual measures of standardizing the numerator and denominator ratings weren't changed. That is actually part of the appeal of the WeRateDogs account and fully intentional.\n",
    "\n",
    "In order to adhere to principles of data tidiness though I elected to combine the three datasets into one instead of three separate, cleaned datasets, since all the information was relevant tweet information. After gatherig, assessing, and cleaning, I stored the clean master dataset as `twitter_archive_master.csv`.\n",
    "\n",
    "Several insights were gleaned from working with the information, all of which can be found in my actionable report."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
